{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkbQxE2geZR+YAKSKBbVUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmeendez8/mmdet_benchmark/blob/main/mmdet_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mmdet model comparison\n",
        "\n",
        "In this notebook we will show a simple way of running the benchmark script on different mmdetection models and compare their results.\n",
        "\n",
        "Work is inspired by [Timm and Jeremy Howard notebook](https://www.kaggle.com/code/jhoward/which-image-models-are-best) where they compare Timm Image Model performance.\n",
        "\n",
        "## Install torch and mmdetection\n",
        "\n",
        "Note benchmarking is carried on GPU so be sure to enable GPU for this notebook."
      ],
      "metadata": {
        "id": "TnCCVaBHHTTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latest torch version supported by mmengine at this moment\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install mmengine==0.10.2\n",
        "!pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html\n",
        "!pip install mmdet==3.3.0"
      ],
      "metadata": {
        "id": "CbMXmNr-HHfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download COCO val data"
      ],
      "metadata": {
        "id": "St69QJr4IPH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the coco directory and cd into it\n",
        "!mkdir coco\n",
        "%cd coco\n",
        "\n",
        "# Create the images directory and cd into it\n",
        "!mkdir images\n",
        "%cd images\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip val2017.zip\n",
        "!rm val2017.zip\n",
        "\n",
        "# Go back to the coco directory\n",
        "%cd ../\n",
        "\n",
        "# Download the annotation zip files\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip\n",
        "\n",
        "# Delete extra files\n",
        "!find annotations -type f ! -name 'instances_val2017.json' -delete\n",
        "\n",
        "# Clean up the zip files\n",
        "!rm annotations_trainval2017.zip\n",
        "\n",
        "# Go back to root directory\n",
        "%cd ../\n"
      ],
      "metadata": {
        "id": "MiKv55OYICWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone benchmark metadata\n",
        "\n",
        "I have created a simple GH repo where I saved the metadata files for the benchmarking. These are mostly model config files from the mmdet repo that I have adapted to use a fixed image size and single gpu.\n",
        "\n",
        "The structure of this repo is very simple:\n",
        "\n",
        "- `metadata.json`: contains simple metadata about each model like family, config file path, model mAP metric, training image size, ...\n",
        "- `models/`: directory where all model config files are stored\n",
        "- `models/shared_config.py`: contains the dataset config that will be used for all models. COCO val data with image resized to 640x640 px-\n",
        "\n",
        "For adding a new model to the benchmark you just need to add a new entry to the `metadata.json` file and the config file of the model to the `models` folder."
      ],
      "metadata": {
        "id": "Ard8rUXVI1Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mmeendez8/mmdet_benchmark.git"
      ],
      "metadata": {
        "id": "2phoHfqVI5Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking Code\n",
        "\n",
        "This code has been created from the official mmdet benchmark script that you can check in: https://github.com/open-mmlab/mmdetection/blob/main/tools/analysis_tools/benchmark.py"
      ],
      "metadata": {
        "id": "utaP3WjZNnHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from mmengine import MMLogger\n",
        "from mmengine.config import Config\n",
        "from mmengine.registry import init_default_scope\n",
        "from mmdet.utils.benchmark import InferenceBenchmark\n",
        "\n",
        "def benchmark(\n",
        "    config: str,\n",
        "    checkpoint: str,\n",
        "    work_dir: str,\n",
        "    repeat_num: int = 1,\n",
        "    max_iter: int = 2000,\n",
        "    log_interval: int = 50,\n",
        "    num_warmup: int = 5,\n",
        "    fuse_conv_bn: bool = False,\n",
        "    model_name: str = \"mmdet\",\n",
        "):\n",
        "    cfg = Config.fromfile(config)\n",
        "\n",
        "    init_default_scope(cfg.get(\"default_scope\", \"mmdet\"))\n",
        "\n",
        "    log_file = osp.join(work_dir, \"benchmark.log\")\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "\n",
        "    logger = MMLogger.get_instance(model_name, log_file=log_file, log_level=\"INFO\")\n",
        "\n",
        "    benchmark = InferenceBenchmark(\n",
        "        cfg=cfg,\n",
        "        checkpoint=checkpoint,\n",
        "        distributed=False,\n",
        "        is_fuse_conv_bn=fuse_conv_bn,\n",
        "        max_iter=max_iter,\n",
        "        log_interval=log_interval,\n",
        "        num_warmup=num_warmup,\n",
        "        logger=logger,\n",
        "    )\n",
        "\n",
        "    benchmark.run(repeat_num)"
      ],
      "metadata": {
        "id": "lMO0dGOLMIX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will define a simple function that parses the log file output from the benchmark script and extracts fps and cuda memory metrics."
      ],
      "metadata": {
        "id": "iciAGpxCOX14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def parse_log_results(log_file):\n",
        "    with open(log_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    last_lines = lines[-3:]\n",
        "\n",
        "    fps_pattern = r\"Overall fps: (\\d+\\.\\d+)\"\n",
        "    cuda_memory_pattern = r\"cuda memory: (\\d+)\"\n",
        "\n",
        "    overall_fps = None\n",
        "    cuda_memory = None\n",
        "\n",
        "    # Search for the values in the last three lines\n",
        "    for line in last_lines:\n",
        "        fps_match = re.search(fps_pattern, line)\n",
        "        cuda_memory_match = re.search(cuda_memory_pattern, line)\n",
        "\n",
        "        if fps_match:\n",
        "            overall_fps = float(fps_match.group(1))\n",
        "        if cuda_memory_match:\n",
        "            cuda_memory = int(cuda_memory_match.group(1))\n",
        "\n",
        "    # Clean cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return overall_fps, cuda_memory\n"
      ],
      "metadata": {
        "id": "4X3iWhELM8Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Benchmark"
      ],
      "metadata": {
        "id": "yr_IDxdsOs5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_folder = \"mmdet_benchmark/\"\n",
        "metadata_file = \"metadata.json\"\n",
        "models_dir = \"models\"\n",
        "\n",
        "# Benchmark variables\n",
        "MAX_ITER = 500\n",
        "LOG_INTERVAL = 500"
      ],
      "metadata": {
        "id": "tLGuTJmROkIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(osp.join(metadata_folder, metadata_file)) as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "for family, models in metadata.items():\n",
        "    for i, model in enumerate(models):\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Benchmarking model {model['name']}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        model_dir = osp.join(metadata_folder, models_dir, model['name'])\n",
        "\n",
        "        config = osp.join(model_dir, \"config.py\")\n",
        "        checkpoint = model['checkpoint']\n",
        "\n",
        "        benchmark(config=config, checkpoint=checkpoint, max_iter=MAX_ITER, work_dir=model_dir, log_interval=LOG_INTERVAL, model_name=model['name'])\n",
        "        fps, cuda_memory = parse_log_results(osp.join(model_dir, \"benchmark.log\"))\n",
        "\n",
        "        # Remove the log file\n",
        "        # os.remove(osp.join(model_dir, \"benchmark.log\"))\n",
        "\n",
        "        print(f\"Overall FPS: {fps}\")\n",
        "        print(f\"CUDA memory: {cuda_memory} MB\")\n",
        "\n",
        "        metadata[family][i]['fps'] = fps\n",
        "        metadata[family][i]['cuda_memory'] = cuda_memory"
      ],
      "metadata": {
        "id": "9oEImKHtO33l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results\n",
        "\n"
      ],
      "metadata": {
        "id": "6i0mCNOyRyLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for family, models in metadata.items():\n",
        "    for model in models:\n",
        "        tmp = model.copy()\n",
        "        tmp['family'] = family\n",
        "        tmp[\"img_size\"] = model[\"img_size\"][1] * model[\"img_size\"][0]\n",
        "        data.append(tmp)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "id": "mVyLEznfRGGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the results for the performance benchmark. We can see:\n",
        "- x axis shows fps to process a single image\n",
        "- y axis shows map metric (directly copied from mmdet metrics)\n",
        "- the size of each bubble is proportional to the size of images used in training\n",
        "- the color shows what family the architecture is form"
      ],
      "metadata": {
        "id": "o_ddftoGTgb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import torch\n",
        "\n",
        "w, h = 1000,800\n",
        "\n",
        "def show_all(df, title, x):\n",
        "    return px.scatter(df, width=w, height=h, size=df[\"img_size\"], title=title,\n",
        "        x=x,  y='box_AP', log_x=False, color='family', hover_name='name')\n",
        "\n",
        "title = f\"Inference speed vs. mAP [{torch.cuda.get_device_name(torch.cuda.current_device())}]\"\n",
        "show_all(df, title, \"fps\")"
      ],
      "metadata": {
        "id": "rjyBVi58Tpgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also show the cuda memory used by the models in the x axis:"
      ],
      "metadata": {
        "id": "_0MoSaHGl1ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = f\"Cuda memory vs. mAP [{torch.cuda.get_device_name(torch.cuda.current_device())}]\"\n",
        "show_all(df, title, \"cuda_memory\")"
      ],
      "metadata": {
        "id": "EBPZ1yQ0lzyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "suSz5mStnCP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}